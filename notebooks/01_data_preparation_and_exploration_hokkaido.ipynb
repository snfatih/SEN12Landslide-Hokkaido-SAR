{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOHDnLGRoJlm"
      },
      "source": [
        "\n",
        "#Dataset Preparation\n",
        " ‚ö†Ô∏è\n",
        "**Purpose: Prepare and validate the dataset by downloading, organizing, extracting, and verifying data integrity before proceeding to any deep learning model training.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4kN2QG9oNx0"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1) Config\n",
        "# Sets up the main configuration\n",
        "# =========================\n",
        "from pathlib import Path\n",
        "\n",
        "# Sensor folders (same structure for every user)\n",
        "SENSORS = [\"s1asc\", \"s1dsc\", \"s2asc\", \"s2dsc\"]\n",
        "\n",
        "# Base directory in user's Google Drive\n",
        "BASE = Path(\"/content/drive/MyDrive/\")\n",
        "\n",
        "# Where downloaded archives will be stored\n",
        "ARCHIVE_DIRNAME = \"archives\"\n",
        "\n",
        "# Where extracted data will be stored\n",
        "EXTRACT_DIRNAME = \"extracted\"\n",
        "\n",
        "# File types used for verification\n",
        "VERIFY_EXTS = (\".nc\", \".tif\", \".tiff\", \".npz\")\n",
        "\n",
        "print(\"BASE:\", BASE)\n",
        "print(\"SENSORS:\", SENSORS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNzs7CG0pneB"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 2) Drive Mount\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "def ensure_drive_mount(preferred=\"/content/drive\"):\n",
        "    \"\"\"\n",
        "    Safely mount Google Drive.\n",
        "    - If already mounted ‚Üí do nothing\n",
        "    - If mountpoint is not empty ‚Üí use alternative mountpoint\n",
        "    \"\"\"\n",
        "    # Drive already mounted\n",
        "    if os.path.isdir(preferred) and \"MyDrive\" in os.listdir(preferred):\n",
        "        print(\"‚úÖ Drive already mounted at\", preferred)\n",
        "        return preferred\n",
        "\n",
        "    # Try preferred mountpoint\n",
        "    try:\n",
        "        os.makedirs(preferred, exist_ok=True)\n",
        "        if os.listdir(preferred):  # not empty ‚Üí unsafe\n",
        "            raise RuntimeError(\"Mountpoint not empty\")\n",
        "\n",
        "        drive.mount(preferred)\n",
        "        return preferred\n",
        "\n",
        "    except Exception:\n",
        "        # Fallback mountpoint\n",
        "        alt = \"/content/drive_mount\"\n",
        "        os.makedirs(alt, exist_ok=True)\n",
        "        print(f\"‚ö†Ô∏è Using alternative mountpoint: {alt}\")\n",
        "        drive.mount(alt)\n",
        "        return alt\n",
        "\n",
        "MOUNTPOINT = ensure_drive_mount()\n",
        "print(\"MOUNTPOINT:\", MOUNTPOINT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Lh5dQk7spNO"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 3) Create Folder Structure\n",
        "# =========================\n",
        "for s in SENSORS:\n",
        "    (BASE / s / ARCHIVE_DIRNAME).mkdir(parents=True, exist_ok=True)\n",
        "    (BASE / s / EXTRACT_DIRNAME).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Folder structure created under:\", BASE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM1g8qbYsxlI"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 4) HuggingFace Login\n",
        "# =========================\n",
        "\n",
        "!pip -q install -U huggingface_hub==0.23.4\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()  # secure interactive login (no token stored in notebook)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiXgdBJJp3eh"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 5a) Download from HuggingFace Hub\n",
        "# =========================\n",
        "from huggingface_hub import hf_hub_download\n",
        "from pathlib import Path\n",
        "\n",
        "# CHANGE THESE\n",
        "repo_id = \"OWNER/REPO\"       # e.g. \"username/dataset-name\"\n",
        "repo_type = \"dataset\"        # or \"model\"\n",
        "\n",
        "FILES_BY_SENSOR = {\n",
        "    \"s1asc\": [\"s1asc_part01.tar.gz\", \"s1asc_part02.tar.gz\"],\n",
        "    \"s1dsc\": [\"s1dsc_part01.tar.gz\"],\n",
        "    \"s2asc\": [\"s2asc_part01.tar.gz\"],\n",
        "    \"s2dsc\": [\"s2dsc_part01.tar.gz\"],\n",
        "}\n",
        "\n",
        "def download_if_missing(sensor, filename):\n",
        "    out_dir = BASE / sensor / ARCHIVE_DIRNAME\n",
        "    out_path = out_dir / filename\n",
        "\n",
        "    if out_path.exists() and out_path.stat().st_size > 0:\n",
        "        print(\"‚úÖ already exists:\", out_path.name)\n",
        "        return out_path\n",
        "\n",
        "    print(\"‚¨áÔ∏è downloading:\", filename)\n",
        "    tmp = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        repo_type=repo_type,\n",
        "        filename=filename\n",
        "    )\n",
        "    out_path.write_bytes(Path(tmp).read_bytes())\n",
        "    print(\"‚úÖ saved:\", out_path)\n",
        "    return out_path\n",
        "\n",
        "for sensor, files in FILES_BY_SENSOR.items():\n",
        "    for f in files:\n",
        "        download_if_missing(sensor, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76W6b1yzqBnk"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 6) Safe Extract (idempotent)\n",
        "# Defines extraction utilities using tarfile\n",
        "# =========================\n",
        "\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "\n",
        "def safe_extract_tar(tar_path: Path, out_dir: Path):\n",
        "    marker = out_dir / f\"._EXTRACTED_{tar_path.name}.done\"\n",
        "    if marker.exists():\n",
        "        print(\"‚úÖ skip:\", tar_path.name)\n",
        "        return\n",
        "\n",
        "    print(\"üì¶ extracting:\", tar_path.name)\n",
        "    with tarfile.open(tar_path, \"r:*\") as tf:\n",
        "        try:\n",
        "            tf.extractall(path=out_dir, filter=\"data\")  # secure (Python ‚â•3.12)\n",
        "        except TypeError:\n",
        "            tf.extractall(path=out_dir)\n",
        "\n",
        "    marker.write_text(\"ok\")\n",
        "    print(\"‚úÖ done:\", tar_path.name)\n",
        "\n",
        "def extract_sensor(sensor):\n",
        "    arch_dir = BASE / sensor / ARCHIVE_DIRNAME\n",
        "    out_dir  = BASE / sensor / EXTRACT_DIRNAME\n",
        "\n",
        "    archives = (\n",
        "        list(arch_dir.glob(\"*.tar\")) +\n",
        "        list(arch_dir.glob(\"*.tar.gz\")) +\n",
        "        list(arch_dir.glob(\"*.tgz\"))\n",
        "    )\n",
        "\n",
        "    if not archives:\n",
        "        print(f\"‚ö†Ô∏è {sensor}: no archives found\")\n",
        "        return\n",
        "\n",
        "    for a in sorted(archives):\n",
        "        safe_extract_tar(a, out_dir)\n",
        "\n",
        "for s in SENSORS:\n",
        "    extract_sensor(s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA2YRNihqGcz"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 7) Verification & Report\n",
        "# Scans extracted folders and prints a report.\n",
        "# =========================\n",
        "import time\n",
        "\n",
        "def quick_stats(folder, exts=VERIFY_EXTS, sample_n=3):\n",
        "    t0 = time.time()\n",
        "    counts = {e: 0 for e in exts}\n",
        "    size = 0\n",
        "    samples = []\n",
        "\n",
        "    for ext in exts:\n",
        "        for p in folder.rglob(f\"*{ext}\"):\n",
        "            try:\n",
        "                st = p.stat()\n",
        "            except FileNotFoundError:\n",
        "                continue\n",
        "            counts[ext] += 1\n",
        "            size += st.st_size\n",
        "            if len(samples) < sample_n:\n",
        "                samples.append(str(p))\n",
        "\n",
        "    return counts, size, samples, time.time() - t0\n",
        "\n",
        "print(\"\\n=========== DATA VERIFICATION ===========\")\n",
        "for s in SENSORS:\n",
        "    out = BASE / s / EXTRACT_DIRNAME\n",
        "    if not out.exists():\n",
        "        print(f\"{s}: ‚ùå missing extracted directory\")\n",
        "        continue\n",
        "\n",
        "    counts, size, samples, secs = quick_stats(out)\n",
        "    gb = size / (1024**3)\n",
        "\n",
        "    print(f\"\\n{s} | {gb:.2f} GB | scan {secs:.1f}s\")\n",
        "    print(\" counts:\", {k:v for k,v in counts.items() if v})\n",
        "    for x in samples:\n",
        "        print(\"  sample:\", x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1vqdsv4n33D"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 8) Verification & Report\n",
        "# List dataset .nc files\n",
        "# =========================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Sanity check\n",
        "!ls -la /content/drive | head\n",
        "!ls -la /content/drive/MyDrive | head\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/attention_unet/sen12landslidedata\")\n",
        "\n",
        "\n",
        "for folder in [\"s1asc\", \"s1dsc\", \"s2\"]:\n",
        "    path = BASE / folder\n",
        "    nc_files = sorted(path.glob(\"*.nc\"))\n",
        "\n",
        "    print(f\"\\nüìÅ {folder}\")\n",
        "    print(f\"Total .nc files: {len(nc_files)}\")\n",
        "\n",
        "    for f in nc_files[:10]:\n",
        "        print(\"  \", f.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKGX3XcrvsnA"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 10) Path validation\n",
        "# Validate dataset paths\n",
        "# =========================\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Update this to YOUR folder that contains: s1asc/, s1dsc/, s2/\n",
        "BASE = Path(\"/content/drive/MyDrive/attention_unet/sen12landslidedata\")  # <-- change if needed\n",
        "\n",
        "assert str(BASE).startswith(\"/content/drive/\"), \"BASE must point inside Google Drive (/content/drive/MyDrive/...)\"\n",
        "assert os.path.exists(\"/content/drive/MyDrive\"), \"Drive not mounted. Run drive.mount('/content/drive') first.\"\n",
        "\n",
        "S1ASC_DIR = BASE / \"s1asc\"\n",
        "S1DSC_DIR = BASE / \"s1dsc\"\n",
        "S2_DIR    = BASE / \"s2\"\n",
        "\n",
        "assert S1ASC_DIR.exists(), f\"Missing: {S1ASC_DIR}\"\n",
        "assert S1DSC_DIR.exists(), f\"Missing: {S1DSC_DIR}\"\n",
        "assert S2_DIR.exists(),    f\"Missing: {S2_DIR}\"\n",
        "\n",
        "print(\"OK ‚úÖ Found folders\")\n",
        "print(\"s1asc:\", S1ASC_DIR)\n",
        "print(\"s1dsc:\", S1DSC_DIR)\n",
        "print(\"s2   :\", S2_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}